{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 481 images belonging to 5 classes.\n",
      "Found 118 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and apply data augmentation\n",
    "data_dir = 'C:\\Vijay\\Capstone\\Dataset'\n",
    "img_height, img_width = 256, 256\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Define the custom CNN filter model using the AlexNet architecture\n",
    "def custom_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Conv2D(384, (3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(Conv2D(384, (3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "15/15 [==============================] - 51s 3s/step - loss: 2.1398 - accuracy: 0.2339 - val_loss: 1.6103 - val_accuracy: 0.1979\n",
      "Epoch 2/25\n",
      "15/15 [==============================] - 55s 4s/step - loss: 1.5929 - accuracy: 0.2405 - val_loss: 1.5626 - val_accuracy: 0.2708\n",
      "Epoch 3/25\n",
      "15/15 [==============================] - 98s 7s/step - loss: 1.5783 - accuracy: 0.2249 - val_loss: 1.5861 - val_accuracy: 0.2188\n",
      "Epoch 4/25\n",
      "15/15 [==============================] - 109s 7s/step - loss: 1.5496 - accuracy: 0.2673 - val_loss: 2.1580 - val_accuracy: 0.2188\n",
      "Epoch 5/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.6570 - accuracy: 0.2539 - val_loss: 1.5704 - val_accuracy: 0.2917\n",
      "Epoch 6/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.5737 - accuracy: 0.2851 - val_loss: 1.5756 - val_accuracy: 0.2500\n",
      "Epoch 7/25\n",
      "15/15 [==============================] - 50s 3s/step - loss: 1.5518 - accuracy: 0.2918 - val_loss: 1.7457 - val_accuracy: 0.1250\n",
      "Epoch 8/25\n",
      "15/15 [==============================] - 50s 3s/step - loss: 1.5441 - accuracy: 0.2851 - val_loss: 1.6354 - val_accuracy: 0.2604\n",
      "Epoch 9/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.5184 - accuracy: 0.3341 - val_loss: 1.3958 - val_accuracy: 0.5104\n",
      "Epoch 10/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.5140 - accuracy: 0.3675 - val_loss: 1.5125 - val_accuracy: 0.3021\n",
      "Epoch 11/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.5033 - accuracy: 0.3029 - val_loss: 1.3616 - val_accuracy: 0.4062\n",
      "Epoch 12/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.5048 - accuracy: 0.3519 - val_loss: 1.4815 - val_accuracy: 0.4062\n",
      "Epoch 13/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.4571 - accuracy: 0.3942 - val_loss: 1.4106 - val_accuracy: 0.4479\n",
      "Epoch 14/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.3869 - accuracy: 0.4187 - val_loss: 1.3352 - val_accuracy: 0.4583\n",
      "Epoch 15/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.3882 - accuracy: 0.4053 - val_loss: 1.3434 - val_accuracy: 0.4792\n",
      "Epoch 16/25\n",
      "15/15 [==============================] - 51s 3s/step - loss: 1.7899 - accuracy: 0.3438 - val_loss: 1.6470 - val_accuracy: 0.1875\n",
      "Epoch 17/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.4573 - accuracy: 0.3296 - val_loss: 1.3491 - val_accuracy: 0.3958\n",
      "Epoch 18/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.4004 - accuracy: 0.4098 - val_loss: 1.3782 - val_accuracy: 0.4375\n",
      "Epoch 19/25\n",
      "15/15 [==============================] - 48s 3s/step - loss: 1.4072 - accuracy: 0.3808 - val_loss: 1.3289 - val_accuracy: 0.4375\n",
      "Epoch 20/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.3164 - accuracy: 0.4588 - val_loss: 1.3637 - val_accuracy: 0.2188\n",
      "Epoch 21/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.4043 - accuracy: 0.3920 - val_loss: 1.4001 - val_accuracy: 0.3958\n",
      "Epoch 22/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.3365 - accuracy: 0.3942 - val_loss: 1.3809 - val_accuracy: 0.4896\n",
      "Epoch 23/25\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.3459 - accuracy: 0.4388 - val_loss: 1.3592 - val_accuracy: 0.4375\n",
      "Epoch 24/25\n",
      "15/15 [==============================] - 50s 3s/step - loss: 1.3416 - accuracy: 0.4165 - val_loss: 1.3363 - val_accuracy: 0.4896\n",
      "Epoch 25/25\n",
      "15/15 [==============================] - 51s 3s/step - loss: 1.3690 - accuracy: 0.4250 - val_loss: 1.6569 - val_accuracy: 0.4688\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and compile it\n",
    "input_shape = (img_height, img_width, 3)\n",
    "num_classes = 5\n",
    "model = custom_alexnet(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 25\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('fabric_classification_alexnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
